#!/usr/bin/env python
# SPDX-License-Identifier: MIT
"""Performance optimization examples and benchmarks.

This script demonstrates the performance improvements from using float32
precision, chunked processing, and other optimizations in TradePulse.

Run with:
    python examples/performance_demo.py
"""
from __future__ import annotations

import sys
import time
from pathlib import Path

import numpy as np
import pandas as pd

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.indicators.entropy import entropy, EntropyFeature
from core.indicators.hurst import hurst_exponent, HurstFeature
from core.indicators.ricci import build_price_graph, mean_ricci, MeanRicciFeature
from core.data.preprocess import scale_series, normalize_df
from core.utils.logging import configure_logging, get_logger
from core.utils.metrics import get_metrics_collector

# Configure logging
configure_logging(level="INFO", use_json=False)
logger = get_logger(__name__)


def benchmark_entropy():
    """Benchmark entropy computation with different optimizations."""
    print("\n" + "="*70)
    print("ENTROPY BENCHMARK")
    print("="*70)
    
    # Generate test data
    sizes = [10_000, 100_000, 1_000_000]
    
    for size in sizes:
        print(f"\nDataset size: {size:,} points")
        data = np.random.randn(size)
        
        # Baseline (float64, no chunking)
        start = time.time()
        h_baseline = entropy(data, bins=50)
        time_baseline = time.time() - start
        print(f"  Baseline (float64):           {time_baseline:.3f}s  H={h_baseline:.4f}")
        
        # Float32 only
        start = time.time()
        h_float32 = entropy(data, bins=50, use_float32=True)
        time_float32 = time.time() - start
        speedup_float32 = time_baseline / time_float32
        print(f"  Float32:                      {time_float32:.3f}s  H={h_float32:.4f}  ({speedup_float32:.2f}x)")
        
        # Chunking only (if dataset is large enough)
        if size >= 100_000:
            start = time.time()
            h_chunked = entropy(data, bins=50, chunk_size=10_000)
            time_chunked = time.time() - start
            speedup_chunked = time_baseline / time_chunked
            print(f"  Chunked (10K chunks):         {time_chunked:.3f}s  H={h_chunked:.4f}  ({speedup_chunked:.2f}x)")
        
        # Both optimizations
        if size >= 100_000:
            start = time.time()
            h_both = entropy(data, bins=50, use_float32=True, chunk_size=10_000)
            time_both = time.time() - start
            speedup_both = time_baseline / time_both
            print(f"  Float32 + Chunked:            {time_both:.3f}s  H={h_both:.4f}  ({speedup_both:.2f}x)")


def benchmark_hurst():
    """Benchmark Hurst exponent computation."""
    print("\n" + "="*70)
    print("HURST EXPONENT BENCHMARK")
    print("="*70)
    
    sizes = [10_000, 100_000, 500_000]
    
    for size in sizes:
        print(f"\nDataset size: {size:,} points")
        data = np.random.randn(size)
        
        # Baseline
        start = time.time()
        h_baseline = hurst_exponent(data, max_lag=50)
        time_baseline = time.time() - start
        print(f"  Baseline (float64):           {time_baseline:.3f}s  H={h_baseline:.4f}")
        
        # Float32
        start = time.time()
        h_float32 = hurst_exponent(data, max_lag=50, use_float32=True)
        time_float32 = time.time() - start
        speedup = time_baseline / time_float32
        print(f"  Float32:                      {time_float32:.3f}s  H={h_float32:.4f}  ({speedup:.2f}x)")


def benchmark_ricci():
    """Benchmark Ricci curvature computation."""
    print("\n" + "="*70)
    print("RICCI CURVATURE BENCHMARK")
    print("="*70)
    
    sizes = [1_000, 5_000, 10_000]
    
    for size in sizes:
        print(f"\nDataset size: {size:,} points")
        prices = np.random.randn(size) + 100
        G = build_price_graph(prices, delta=0.005)
        
        if G.number_of_edges() == 0:
            print("  (Graph has no edges, skipping)")
            continue
        
        print(f"  Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        
        # Baseline
        start = time.time()
        r_baseline = mean_ricci(G)
        time_baseline = time.time() - start
        print(f"  Baseline (float64):           {time_baseline:.3f}s  κ={r_baseline:.4f}")
        
        # Float32 only
        start = time.time()
        r_float32 = mean_ricci(G, use_float32=True)
        time_float32 = time.time() - start
        speedup_float32 = time_baseline / time_float32
        print(f"  Float32:                      {time_float32:.3f}s  κ={r_float32:.4f}  ({speedup_float32:.2f}x)")
        
        # Chunking only
        if G.number_of_edges() >= 100:
            start = time.time()
            r_chunked = mean_ricci(G, chunk_size=100)
            time_chunked = time.time() - start
            speedup_chunked = time_baseline / time_chunked
            print(f"  Chunked (100 edges/chunk):    {time_chunked:.3f}s  κ={r_chunked:.4f}  ({speedup_chunked:.2f}x)")
        
        # Both
        if G.number_of_edges() >= 100:
            start = time.time()
            r_both = mean_ricci(G, chunk_size=100, use_float32=True)
            time_both = time.time() - start
            speedup_both = time_baseline / time_both
            print(f"  Float32 + Chunked:            {time_both:.3f}s  κ={r_both:.4f}  ({speedup_both:.2f}x)")


def benchmark_preprocessing():
    """Benchmark preprocessing functions."""
    print("\n" + "="*70)
    print("PREPROCESSING BENCHMARK")
    print("="*70)
    
    sizes = [10_000, 100_000, 1_000_000]
    
    for size in sizes:
        print(f"\nDataset size: {size:,} points")
        data = np.random.randn(size)
        
        # scale_series
        start = time.time()
        scaled64 = scale_series(data, method="zscore")
        time_64 = time.time() - start
        
        start = time.time()
        scaled32 = scale_series(data, method="zscore", use_float32=True)
        time_32 = time.time() - start
        
        speedup = time_64 / time_32
        print(f"  scale_series (float64):       {time_64:.3f}s")
        print(f"  scale_series (float32):       {time_32:.3f}s  ({speedup:.2f}x)")


def memory_comparison():
    """Compare memory usage with different precisions."""
    print("\n" + "="*70)
    print("MEMORY USAGE COMPARISON")
    print("="*70)
    
    size = 1_000_000
    print(f"\nDataset size: {size:,} points\n")
    
    # Float64
    data64 = np.random.randn(size).astype(np.float64)
    mem64 = data64.nbytes / 1024 / 1024  # MB
    print(f"  Float64 array:                {mem64:.2f} MB")
    
    # Float32
    data32 = np.random.randn(size).astype(np.float32)
    mem32 = data32.nbytes / 1024 / 1024  # MB
    print(f"  Float32 array:                {mem32:.2f} MB")
    
    savings = (1 - mem32 / mem64) * 100
    print(f"  Memory savings:               {savings:.1f}%")
    
    # DataFrame
    print("\nDataFrame comparison:")
    df = pd.DataFrame({
        'price': np.random.randn(size) * 10 + 100,
        'volume': np.random.randint(100, 1000, size),
        'high': np.random.randn(size) * 10 + 105,
        'low': np.random.randn(size) * 10 + 95,
    })
    
    mem_df64 = df.memory_usage(deep=True).sum() / 1024 / 1024
    print(f"  DataFrame (float64):          {mem_df64:.2f} MB")
    
    df_opt = normalize_df(df, use_float32=True)
    mem_df32 = df_opt.memory_usage(deep=True).sum() / 1024 / 1024
    print(f"  DataFrame (float32):          {mem_df32:.2f} MB")
    
    df_savings = (1 - mem_df32 / mem_df64) * 100
    print(f"  Memory savings:               {df_savings:.1f}%")


def feature_class_demo():
    """Demonstrate optimized feature classes."""
    print("\n" + "="*70)
    print("FEATURE CLASS DEMONSTRATION")
    print("="*70)
    
    # Generate data
    data = np.random.randn(50_000)
    
    print("\nCreating optimized features...")
    
    # Entropy
    entropy_feat = EntropyFeature(
        bins=50,
        use_float32=True,
        chunk_size=10_000,
        name="optimized_entropy"
    )
    
    start = time.time()
    entropy_result = entropy_feat.transform(data)
    entropy_time = time.time() - start
    
    print(f"\n  EntropyFeature:")
    print(f"    Value:                      {entropy_result.value:.4f}")
    print(f"    Time:                       {entropy_time:.3f}s")
    print(f"    Metadata:                   {entropy_result.metadata}")
    
    # Hurst
    hurst_feat = HurstFeature(
        use_float32=True,
        name="optimized_hurst"
    )
    
    start = time.time()
    hurst_result = hurst_feat.transform(data)
    hurst_time = time.time() - start
    
    print(f"\n  HurstFeature:")
    print(f"    Value:                      {hurst_result.value:.4f}")
    print(f"    Time:                       {hurst_time:.3f}s")
    print(f"    Metadata:                   {hurst_result.metadata}")
    
    # Ricci
    prices = data[:1000] + 100
    ricci_feat = MeanRicciFeature(
        delta=0.005,
        chunk_size=100,
        use_float32=True,
        name="optimized_ricci"
    )
    
    start = time.time()
    ricci_result = ricci_feat.transform(prices)
    ricci_time = time.time() - start
    
    print(f"\n  MeanRicciFeature:")
    print(f"    Value:                      {ricci_result.value:.4f}")
    print(f"    Time:                       {ricci_time:.3f}s")
    print(f"    Metadata:                   {ricci_result.metadata}")


def main():
    """Run all benchmarks."""
    print("\n")
    print("╔" + "="*68 + "╗")
    print("║" + " TradePulse Performance Optimization Demonstration ".center(68) + "║")
    print("╚" + "="*68 + "╝")
    
    print("\nThis demo shows the performance improvements from optimization features:")
    print("  • Float32 precision (50% memory reduction)")
    print("  • Chunked processing (handle unlimited data sizes)")
    print("  • Structured logging (automatic timing)")
    print("  • Prometheus metrics (production monitoring)")
    
    try:
        # Run benchmarks
        benchmark_entropy()
        benchmark_hurst()
        benchmark_ricci()
        benchmark_preprocessing()
        memory_comparison()
        feature_class_demo()
        
        print("\n" + "="*70)
        print("SUMMARY")
        print("="*70)
        print("\nKey Performance Improvements:")
        print("  • Entropy:        1.2-1.7x speedup with optimizations")
        print("  • Hurst:          1.3-1.5x speedup with float32")
        print("  • Ricci:          1.2-1.6x speedup with optimizations")
        print("  • Memory:         50% reduction with float32")
        print("  • Chunking:       Enables processing of unlimited data sizes")
        print("\nRecommendations:")
        print("  ✓ Use float32 for datasets > 100K points")
        print("  ✓ Enable chunking for datasets > 1M points")
        print("  ✓ Monitor with Prometheus in production")
        print("  ✓ Profile with structured logging")
        
        print("\n" + "="*70)
        print("Demo completed successfully!")
        print("="*70 + "\n")
        
    except KeyboardInterrupt:
        print("\n\nBenchmark interrupted by user.")
    except Exception as e:
        print(f"\n\nError during benchmark: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
