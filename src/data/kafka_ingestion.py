"""Kafka ingestion service with exactly-once semantics and hot symbol caching."""

from __future__ import annotations

import asyncio
import json
import logging
import ssl
import time
from collections import OrderedDict
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any, Callable, Dict, Iterable, Mapping, Protocol, Sequence

from core.data.models import InstrumentType, PriceTick
from core.messaging.idempotency import (
    EventIdempotencyStore,
    InMemoryEventIdempotencyStore,
)

logger = logging.getLogger(__name__)


class TickDecoder(Protocol):
    """Decode raw Kafka payloads into :class:`PriceTick` instances."""

    def __call__(
        self,
        *,
        key: bytes | None,
        value: bytes,
        headers: Mapping[str, str],
    ) -> PriceTick: ...


class TickBatchHandler(Protocol):
    """Handle decoded ticks in-order."""

    async def __call__(
        self, ticks: Sequence[PriceTick]
    ) -> None:  # pragma: no cover - protocol
        ...


class LagHandler(Protocol):
    """Handle lag reconciliation reports."""

    async def __call__(
        self, report: "LagReport"
    ) -> None:  # pragma: no cover - protocol
        ...


@dataclass(frozen=True)
class KafkaIngestionConfig:
    """Configuration driving :class:`KafkaIngestionService`."""

    topic: str
    bootstrap_servers: str
    group_id: str
    client_id: str = "tradepulse-kafka-ingestor"
    transactional_id: str = "tradepulse-kafka-ingestor"
    poll_timeout_ms: int = 1000
    max_batch_size: int = 500
    linger_ms: int = 5
    session_timeout_ms: int = 45000
    max_poll_interval_ms: int = 300000
    fetch_max_bytes: int = 5 * 1024 * 1024
    auto_offset_reset: str = "earliest"
    isolation_level: str = "read_committed"
    security_protocol: str = "SSL"
    ssl_cafile: str | None = None
    ssl_certfile: str | None = None
    ssl_keyfile: str | None = None
    sasl_mechanism: str | None = "PLAIN"
    sasl_username: str | None = None
    sasl_password: str | None = None
    dedupe_ttl_seconds: int = 3600
    hot_cache_max_entries: int = 256
    hot_cache_ttl_seconds: int = 30
    hot_cache_max_ticks: int = 512
    hot_cache_flush_size: int = 64
    lag_detection_threshold: int = 100
    lag_report_interval_seconds: float = 5.0
    reconcile_seek_on_gap: bool = True


@dataclass(frozen=True)
class LagRecord:
    """Summary of lag metrics for a single partition."""

    topic: str
    partition: int
    expected_offset: int
    observed_offset: int
    lag: int
    reason: str = "backlog"


@dataclass(frozen=True)
class LagReport:
    """Aggregate lag report generated by the reconciliation loop."""

    generated_at: datetime
    records: tuple[LagRecord, ...]


@dataclass
class _HotSymbolEntry:
    ticks: list[PriceTick] = field(default_factory=list)
    last_seen: float = field(default_factory=time.monotonic)


@dataclass(frozen=True)
class HotSymbolSnapshot:
    """Snapshot of cached ticks for a hot symbol."""

    symbol: str
    venue: str
    instrument_type: InstrumentType
    ticks: tuple[PriceTick, ...]
    last_seen: datetime


class HotSymbolCache:
    """LRU cache retaining recent ticks for the hottest symbols."""

    def __init__(
        self,
        *,
        max_entries: int,
        ttl_seconds: float,
        max_ticks: int,
        flush_size: int,
        clock: Callable[[], float] | None = None,
    ) -> None:
        if max_entries <= 0:
            raise ValueError("max_entries must be strictly positive")
        if ttl_seconds <= 0:
            raise ValueError("ttl_seconds must be strictly positive")
        if max_ticks <= 0:
            raise ValueError("max_ticks must be strictly positive")
        if flush_size <= 0:
            raise ValueError("flush_size must be strictly positive")
        self._max_entries = max_entries
        self._ttl_seconds = ttl_seconds
        self._max_ticks = max_ticks
        self._flush_size = flush_size
        self._clock = clock or time.monotonic
        self._entries: OrderedDict[tuple[str, str, InstrumentType], _HotSymbolEntry] = (
            OrderedDict()
        )

    def update(self, tick: PriceTick) -> list[HotSymbolSnapshot]:
        key = (tick.symbol, tick.venue, tick.instrument_type)
        entry = self._entries.get(key)
        now = self._clock()
        flushed: list[HotSymbolSnapshot] = []
        if entry is None:
            entry = _HotSymbolEntry()
            self._entries[key] = entry
        else:
            self._entries.move_to_end(key)
        entry.ticks.append(tick)
        entry.last_seen = now
        if len(entry.ticks) > self._max_ticks:
            overflow = entry.ticks[: -self._max_ticks]
            entry.ticks = entry.ticks[-self._max_ticks :]
            flushed.append(
                HotSymbolSnapshot(
                    symbol=key[0],
                    venue=key[1],
                    instrument_type=key[2],
                    ticks=tuple(overflow),
                    last_seen=datetime.fromtimestamp(now, tz=timezone.utc),
                )
            )
        if len(entry.ticks) >= self._flush_size:
            flushed.append(
                HotSymbolSnapshot(
                    symbol=key[0],
                    venue=key[1],
                    instrument_type=key[2],
                    ticks=tuple(entry.ticks),
                    last_seen=datetime.fromtimestamp(now, tz=timezone.utc),
                )
            )
            entry.ticks = []
        flushed.extend(self._evict_stale(now))
        return flushed

    def _evict_stale(self, now: float) -> list[HotSymbolSnapshot]:
        flushed: list[HotSymbolSnapshot] = []
        while self._entries and (len(self._entries) > self._max_entries):
            key, entry = self._entries.popitem(last=False)
            if entry.ticks:
                flushed.append(
                    HotSymbolSnapshot(
                        symbol=key[0],
                        venue=key[1],
                        instrument_type=key[2],
                        ticks=tuple(entry.ticks),
                        last_seen=datetime.fromtimestamp(now, tz=timezone.utc),
                    )
                )
        expired_keys: list[tuple[str, str, InstrumentType]] = []
        for key, entry in self._entries.items():
            if now - entry.last_seen >= self._ttl_seconds and entry.ticks:
                flushed.append(
                    HotSymbolSnapshot(
                        symbol=key[0],
                        venue=key[1],
                        instrument_type=key[2],
                        ticks=tuple(entry.ticks),
                        last_seen=datetime.fromtimestamp(
                            entry.last_seen, tz=timezone.utc
                        ),
                    )
                )
                entry.ticks = []
                expired_keys.append(key)
        for key in expired_keys:
            self._entries.move_to_end(key)
        return flushed

    def snapshot(
        self,
        symbol: str,
        venue: str,
        instrument_type: InstrumentType = InstrumentType.SPOT,
    ) -> HotSymbolSnapshot | None:
        key = (symbol, venue, instrument_type)
        entry = self._entries.get(key)
        if entry is None:
            return None
        return HotSymbolSnapshot(
            symbol=symbol,
            venue=venue,
            instrument_type=instrument_type,
            ticks=tuple(entry.ticks),
            last_seen=datetime.fromtimestamp(entry.last_seen, tz=timezone.utc),
        )

    def drain(self) -> list[HotSymbolSnapshot]:
        drained: list[HotSymbolSnapshot] = []
        while self._entries:
            key, entry = self._entries.popitem(last=False)
            if entry.ticks:
                drained.append(
                    HotSymbolSnapshot(
                        symbol=key[0],
                        venue=key[1],
                        instrument_type=key[2],
                        ticks=tuple(entry.ticks),
                        last_seen=datetime.fromtimestamp(
                            entry.last_seen, tz=timezone.utc
                        ),
                    )
                )
        return drained


@dataclass
class _PartitionState:
    last_processed: int = -1
    expected_offset: int | None = None
    last_lag: int = 0
    last_seen: float = field(default_factory=time.monotonic)
    pending_ticks: list[PriceTick] = field(default_factory=list)
    pending_event_ids: list[str] = field(default_factory=list)
    pending_offset: int | None = None

    def advance(self, offset: int) -> None:
        self.last_processed = offset
        self.expected_offset = offset + 1
        self.last_seen = time.monotonic()
        self.pending_offset = offset + 1


class KafkaIngestionService:
    """Exactly-once Kafka ingestion with deduplication and lag reconciliation."""

    def __init__(
        self,
        config: KafkaIngestionConfig,
        *,
        tick_handler: TickBatchHandler | None = None,
        tick_decoder: TickDecoder | None = None,
        idempotency_store: EventIdempotencyStore | None = None,
        lag_handler: LagHandler | None = None,
        consumer: Any | None = None,
        producer: Any | None = None,
        clock: Callable[[], datetime] | None = None,
    ) -> None:
        if not config.topic:
            raise ValueError("topic must be provided")
        self._config = config
        self._tick_handler = tick_handler or self._default_tick_handler
        self._decoder = tick_decoder or _default_tick_decoder
        self._idempotency = idempotency_store or InMemoryEventIdempotencyStore(
            ttl_seconds=config.dedupe_ttl_seconds
        )
        self._lag_handler = lag_handler
        self._consumer = consumer
        self._producer = producer
        self._stop_event = asyncio.Event()
        self._consume_task: asyncio.Task[None] | None = None
        self._lag_task: asyncio.Task[None] | None = None
        self._partition_state: Dict[tuple[str, int], _PartitionState] = {}
        self._clock = clock or (lambda: datetime.now(timezone.utc))
        self._hot_cache = HotSymbolCache(
            max_entries=config.hot_cache_max_entries,
            ttl_seconds=config.hot_cache_ttl_seconds,
            max_ticks=config.hot_cache_max_ticks,
            flush_size=config.hot_cache_flush_size,
        )

    @property
    def hot_cache(self) -> HotSymbolCache:
        return self._hot_cache

    async def start(self) -> None:
        if self._consumer is None:
            self._consumer = await self._build_consumer()
        if self._producer is None:
            self._producer = await self._build_producer()
        await self._producer.start()
        await self._consumer.start()
        self._stop_event.clear()
        self._consume_task = asyncio.create_task(
            self._consume_loop(), name="kafka-ingestion-consumer"
        )
        self._lag_task = asyncio.create_task(
            self._lag_monitor_loop(), name="kafka-ingestion-lag-monitor"
        )

    async def stop(self) -> None:
        self._stop_event.set()
        tasks = [
            task for task in (self._consume_task, self._lag_task) if task is not None
        ]
        for task in tasks:
            task.cancel()
        await asyncio.gather(*tasks, return_exceptions=True)
        self._consume_task = None
        self._lag_task = None
        if self._consumer is not None:
            await self._consumer.stop()
        if self._producer is not None:
            await self._producer.stop()

    async def _consume_loop(self) -> None:
        assert self._consumer is not None and self._producer is not None
        consumer = self._consumer
        producer = self._producer
        while not self._stop_event.is_set():
            try:
                batches = await consumer.getmany(
                    timeout_ms=self._config.poll_timeout_ms,
                    max_records=self._config.max_batch_size,
                )
            except (
                asyncio.CancelledError
            ):  # pragma: no cover - cooperative cancellation
                raise
            except Exception:  # pragma: no cover - defensive logging
                logger.exception("Kafka ingestion poll failed")
                await asyncio.sleep(0.5)
                continue
            if not batches:
                await asyncio.sleep(0)
                continue
            try:
                await producer.begin_transaction()
            except AttributeError as exc:  # pragma: no cover - misconfigured producer
                raise RuntimeError(
                    "Kafka producer does not support transactions"
                ) from exc
            processed_offsets: Dict[Any, int] = {}
            ticks: list[PriceTick] = []
            committable_event_ids: list[str] = []
            current_event_ids: set[str] = set()
            abort_transaction = False
            for tp, messages in batches.items():
                key = (getattr(tp, "topic"), getattr(tp, "partition"))
                state = self._partition_state.setdefault(key, _PartitionState())
                if state.pending_ticks:
                    ticks.extend(state.pending_ticks)
                    committable_event_ids.extend(state.pending_event_ids)
                    current_event_ids.update(state.pending_event_ids)
                    if state.pending_offset is not None:
                        processed_offsets[tp] = state.pending_offset
                    state.pending_ticks = []
                    state.pending_event_ids = []
                for message in messages:
                    headers = _coerce_headers(getattr(message, "headers", []))
                    event_id = (
                        headers.get("event_id") or f"{key[0]}:{key[1]}:{message.offset}"
                    )
                    expected = (
                        state.expected_offset
                        if state.expected_offset is not None
                        else message.offset
                    )
                    if message.offset > expected:
                        logger.warning(
                            "Detected offset gap (topic=%s partition=%s expected=%s observed=%s)",
                            key[0],
                            key[1],
                            expected,
                            message.offset,
                        )
                        if self._config.reconcile_seek_on_gap:
                            await consumer.seek(tp, expected)
                        await producer.abort_transaction()
                        abort_transaction = True
                        await self._emit_lag_report(
                            [
                                LagRecord(
                                    topic=key[0],
                                    partition=key[1],
                                    expected_offset=expected,
                                    observed_offset=message.offset,
                                    lag=message.offset - expected,
                                    reason="gap",
                                )
                            ]
                        )
                        break
                    if message.offset < expected:
                        state.advance(message.offset)
                        processed_offsets[tp] = message.offset + 1
                        continue
                    if (
                        event_id in current_event_ids
                        or self._idempotency.was_processed(event_id)
                    ):
                        state.advance(message.offset)
                        processed_offsets[tp] = message.offset + 1
                        continue
                    try:
                        tick = self._decoder(
                            key=message.key, value=message.value, headers=headers
                        )
                    except Exception:
                        logger.exception("Failed to decode tick payload")
                        state.advance(message.offset)
                        processed_offsets[tp] = message.offset + 1
                        continue
                    ticks.append(tick)
                    state.pending_ticks.append(tick)
                    state.pending_event_ids.append(event_id)
                    current_event_ids.add(event_id)
                    committable_event_ids.append(event_id)
                    state.advance(message.offset)
                    processed_offsets[tp] = message.offset + 1
                if abort_transaction:
                    break
            if abort_transaction:
                continue
            if ticks:
                try:
                    await self._tick_handler(ticks)
                except (
                    Exception
                ):  # pragma: no cover - handler errors logged but offsets still committed
                    logger.exception(
                        "Tick handler failed; offsets will not be committed"
                    )
                    try:
                        await producer.abort_transaction()
                    except Exception:
                        logger.exception("Aborting Kafka transaction failed")
                    await asyncio.sleep(0.5)
                    continue
            try:
                await producer.send_offsets_to_transaction(
                    processed_offsets, self._config.group_id
                )
                await producer.commit_transaction()
                for event_id in committable_event_ids:
                    self._idempotency.mark_processed(event_id)
                for state in self._partition_state.values():
                    state.pending_ticks.clear()
                    state.pending_event_ids.clear()
                    state.pending_offset = None
            except Exception:  # pragma: no cover - transactional failure
                logger.exception("Failed to commit Kafka transaction")
                await asyncio.sleep(0.5)
                continue

    async def _lag_monitor_loop(self) -> None:
        assert self._consumer is not None
        consumer = self._consumer
        interval = max(self._config.lag_report_interval_seconds, 0.5)
        while not self._stop_event.is_set():
            try:
                await asyncio.sleep(interval)
                await self._report_lag(consumer)
            except asyncio.CancelledError:  # pragma: no cover - cancellation path
                raise
            except Exception:
                logger.exception("Lag reconciliation loop failed")

    async def _report_lag(self, consumer: Any) -> None:
        assignment = getattr(consumer, "assignment", lambda: set())()
        if not assignment:
            return
        end_offsets = await consumer.end_offsets(list(assignment))
        reports: list[LagRecord] = []
        for tp in assignment:
            key = (getattr(tp, "topic"), getattr(tp, "partition"))
            state = self._partition_state.get(key)
            expected = (
                state.expected_offset
                if state and state.expected_offset is not None
                else 0
            )
            end_offset = end_offsets.get(tp, 0)
            lag = max(0, end_offset - expected)
            if lag >= self._config.lag_detection_threshold:
                reports.append(
                    LagRecord(
                        topic=key[0],
                        partition=key[1],
                        expected_offset=expected,
                        observed_offset=end_offset,
                        lag=lag,
                        reason="backlog",
                    )
                )
                if self._config.reconcile_seek_on_gap and state:
                    await consumer.seek(tp, expected)
        if reports:
            await self._emit_lag_report(reports)

    async def _emit_lag_report(self, records: Iterable[LagRecord]) -> None:
        if not self._lag_handler:
            return
        report = LagReport(generated_at=self._clock(), records=tuple(records))
        await self._lag_handler(report)

    async def _default_tick_handler(self, ticks: Sequence[PriceTick]) -> None:
        for tick in ticks:
            self._hot_cache.update(tick)

    async def _build_consumer(self) -> Any:
        from aiokafka import AIOKafkaConsumer  # type: ignore

        kwargs = {
            "bootstrap_servers": self._config.bootstrap_servers,
            "group_id": self._config.group_id,
            "client_id": self._config.client_id,
            "enable_auto_commit": False,
            "auto_offset_reset": self._config.auto_offset_reset,
            "isolation_level": self._config.isolation_level,
            "session_timeout_ms": self._config.session_timeout_ms,
            "max_poll_interval_ms": self._config.max_poll_interval_ms,
            "fetch_max_bytes": self._config.fetch_max_bytes,
        }
        kwargs.update(self._build_security_kwargs())
        consumer = AIOKafkaConsumer(self._config.topic, **kwargs)
        return consumer

    async def _build_producer(self) -> Any:
        from aiokafka import AIOKafkaProducer  # type: ignore

        kwargs = {
            "bootstrap_servers": self._config.bootstrap_servers,
            "client_id": self._config.client_id,
            "transactional_id": self._config.transactional_id,
            "enable_idempotence": True,
            "linger_ms": self._config.linger_ms,
        }
        kwargs.update(self._build_security_kwargs())
        producer = AIOKafkaProducer(**kwargs)
        return producer

    def _build_security_kwargs(self) -> Dict[str, Any]:
        protocol = (self._config.security_protocol or "").upper()
        if protocol not in {"SSL", "SASL_SSL"}:
            raise ValueError("Kafka security_protocol must be 'SSL' or 'SASL_SSL'")
        if not self._config.ssl_cafile:
            raise ValueError(
                "ssl_cafile must be configured for secure Kafka connections"
            )
        cafile_path = self._config.ssl_cafile
        context = ssl.create_default_context(cafile=cafile_path)
        if self._config.ssl_certfile and self._config.ssl_keyfile:
            context.load_cert_chain(
                certfile=self._config.ssl_certfile, keyfile=self._config.ssl_keyfile
            )
        elif self._config.ssl_certfile or self._config.ssl_keyfile:
            raise ValueError("ssl_certfile and ssl_keyfile must be provided together")
        kwargs: Dict[str, Any] = {
            "security_protocol": protocol,
            "ssl_context": context,
        }
        if protocol == "SASL_SSL":
            if not self._config.sasl_username or not self._config.sasl_password:
                raise ValueError("SASL credentials must be supplied for SASL_SSL")
            kwargs.update(
                {
                    "sasl_mechanism": self._config.sasl_mechanism or "PLAIN",
                    "sasl_plain_username": self._config.sasl_username,
                    "sasl_plain_password": self._config.sasl_password,
                }
            )
        return kwargs


def _coerce_headers(headers: Iterable[tuple[str, bytes]] | None) -> Dict[str, str]:
    result: Dict[str, str] = {}
    if not headers:
        return result
    for key, value in headers:
        if value is None:
            continue
        try:
            result[key] = value.decode("utf-8")
        except Exception:  # pragma: no cover - defensive logging
            logger.warning("Failed to decode Kafka header %s", key)
    return result


def _default_tick_decoder(
    *, key: bytes | None, value: bytes, headers: Mapping[str, str]
) -> PriceTick:
    if not value:
        raise ValueError("Kafka message payload is empty")
    payload = json.loads(value.decode("utf-8"))
    symbol = payload.get("symbol") or headers.get("symbol")
    if symbol is None and key:
        symbol = key.decode("utf-8")
    venue = payload.get("venue") or headers.get("venue")
    if not symbol or not venue:
        raise ValueError("Tick payload must contain symbol and venue")
    instrument_value = payload.get("instrument_type") or headers.get("instrument_type")
    instrument_type = (
        InstrumentType(str(instrument_value))
        if instrument_value
        else InstrumentType.SPOT
    )
    timestamp = payload.get("timestamp") or headers.get("occurred_at")
    if timestamp is None:
        raise ValueError("Tick payload missing timestamp")
    volume = payload.get("volume")
    trade_id = payload.get("trade_id") or headers.get("trade_id")
    return PriceTick.create(
        symbol=symbol,
        venue=venue,
        price=payload["price"],
        timestamp=timestamp,
        volume=volume,
        instrument_type=instrument_type,
        trade_id=trade_id,
    )
